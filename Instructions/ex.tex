\documentclass[a4paper]{article}
\usepackage{a4wide,latexsym,epsfig,color,xspace}
\usepackage{xspace}

\usepackage{wrapfig}


\usepackage{tikz}
\usetikzlibrary{automata}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}
\tikzstyle{every picture}=[thick]
\tikzstyle{every scope}=[>=latex, node distance=2.3cm]
\tikzstyle{st} = [state, ellipse, minimum size=4mm, inner sep=0.5mm, node distance=2.3cm]


\newcommand{\JTORX}[0]{\textsc{JTorX}\xspace}
\newcommand{\IOCO}[0]{{\ensuremath{\mathbf{ioco}}}\xspace}
\newcommand{\UIOCO}[0]{{\ensuremath{\mathbf{uioco}}}\xspace}
\newcommand{\FAIL}[0]{{\ensuremath{\mathbf{fail}}}\xspace}
\newcommand{\YED}[0]{\textsc{yEd}\xspace}

\newcounter{commentcounter}
\newcommand{\XXX}[1]{%
%  \refstepcounter{commentcounter}%
  \stepcounter{commentcounter}%
  {\footnotesize\bf$^{(\arabic{commentcounter})}$}%
  \marginpar{%
    \footnotesize({\bf{\arabic{commentcounter}}})%
    \footnotesize{#1}%
  }%
}

\usepackage{tweaklist}
\renewcommand{\itemhook}{\setlength{\itemsep}{0pt}
%\setlength{\topsep}{0pt}
}
\renewcommand{\descripthook}{\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}}

\renewcommand{\enumhook}{\setlength{\itemsep}{0pt}\setlength{\partopsep}{0pt}\setlength{\parsep}{0pt}}

%\setlength{\textwidth}{14.9cm}

%\setlength{\oddsidemargin}{1.1cm}
%\setlength{\evensidemargin}{1.1cm}
%\setlength{\topmargin}{0.9cm}
%\setlength{\headsep}{0.0cm}
%\setlength{\textheight}{22cm}
%\setlength{\textwidth}{13.5cm}
%\setlength{\parindent}{0mm}
%\setlength{\parskip}{0.666\baselineskip}
%\setlength{\topsep}{0mm}
%\setlength{\itemsep}{0mm}
%\setlength{\abovedisplayskip}{0mm}
%\setlength{\belowdisplayskip}{0mm}
%\setlength{\abovedisplayshortskip}{0mm}
%\setlength{\belowdisplayshortskip}{0mm}

\usepackage{thumbpdf}
\usepackage[bookmarksnumbered,plainpages,backref,hidelinks]{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Learnlib Practicum}

\author{David Huistra \and Jeroen Meijer}

\date{March 7, 2017}
% \author{Jeroen Meijer (j.j.g.meijer@utwente.nl) \\ \footnotesize{Original author: Axel Belinfante}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% links:
% game animated with flash http://www.plastelina.net/game1.html
% folklore http://www.folklore.ee/Folklore/vol35/voolaid.pdf
% other game http://freeweb.siol.net/danej/riverIQGame.swf

\newcommand{\deadline}{\emph{February 16, 2016, 8:45 hours}\xspace}

\begin{document}

%\pagestyle{empty}
\maketitle
%\thispagestyle{empty}
% \vspace{1.5\baselineskip}

\begin{abstract}
%\textbf{Important: login to the lab computers using course code 217001.}
The goal of this assignment is to practice with the LearnLib framework for automata learning algorithms.
You can do this assignment with the same partner as the previous assignment.

% Note that this assignment requires you to upload a mutant implementation of WGCF
% before the lab class on \deadline.
\end{abstract}

\section*{Preliminaries: Installing stuff.}

\paragraph{Optional: Install Eclipse}
For this assignment we recommend the use of Eclipse. You are however free to compile the code by command lines :)

\paragraph{Optional: Install GraphViz}
The Learnlib provides the option to export the learned models to the GraphDOT format so that they can visualized by the GraphViz application. If you want to visualize the learned models (reccomended), you need to install GraphViz. This can be done from here: \url{http://www.graphviz.org/Download..php}. If you do not install GraphViz, you will see the models in the (textual) GraphDot format.

\emph{Sidenote: For Windows users, please be aware that you will need to add GraphViz to your PATH variable (as explained on the download page, but easily looked over).}

\section*{Part 1. Learning a model of your Wolf-Goat-Cabbage implementation}

\textbf{The assingment}. We will start off this assignment by automatically learning a model of your Wolf-Goat-Cabbage implementation. We will provide you with some basic Learnlib settings for this task. The only task you will need to perform is to write a wrapper for your implementation so that Learnlib will know how to interact with it.  This will be done by implementing Learnlib's SUL interface.

\textbf{Getting to work}. Download the provided basic framework from GitHub\footnote{\url{https://github.com/djhuistra/tt-learnlib-practicum}}. You can import this project into eclipse. Next to the basic framework for using Learnlib (the learner class and the (empty) SUL wrapper class), this also includes a .jar file with the entire Learnlib project. (Including a jar file in eclipse: right click project name  -\textgreater properties  -\textgreater Java Build Path -\textgreater Add External JARs -\textgreater Select learnlib.jar). Make sure that you can successfully run the learner. It should return a model with only one state, as the wrapper has not yet been implemented.

\emph{Sidenote. In the learner class indicate if you have installed graphDOT, so it will produce a graphical model.}

\textbf{Next up}. Include your Wolf-Goat-Cabbage implementation into the project (Including a jar file in eclipse: right click project name-\textgreater properties  -\textgreater Java Build Path  -\textgreater  Add External JARs  -\textgreater  Select implementation jar). Note that in this lab we will not be using the binary encoding of inputs and outputs. 

Now you may start with implementing the SUL wrapper. We advice you to use the WGC implementation as a library jar, not as a runnable jar. So you can import ans use the implemented WGC Model class in your WGCSUL implementation.

We assume the same input and outputs as used in the lecture. So the inputs are one of the set \{"cabbage?", "goat?", "nothing?", "wolf?"\} and each input should return a response that is contained in the set \{"eaten!", "finished!", "init!", "ok!", "retry!"\}. Note that the "ok!" output is new, i.e. we did not see this output in the IOCO lab.

\section*{Part 2. \texttt{SUL} Wrapping}


While it is pretty cool to be able to learn a model by interacting with an implementation, the success of this method depends in practice also on its performance. Is it still feasible to learn the model of a larger implementation in a reasonable amount of time? Increasing the performance of this method is currently an active area of research.

In this part we want you to get some insight into one of the bottlenecks of the process: interaction with the SUL. Each interaction with the SUL is considered expensive, and therefore if possible we want to limit the amount of interaction.
 
First we want to gain some insight into the interaction taking place in the learning process. Therefore we want to wrap our \texttt{SUL} in a \texttt{SymbolCounterSUL}\footnote{\url{https://github.com/LearnLib/learnlib/blob/develop/core/src/main/java/de/learnlib/oracles/SymbolCounterSUL.java}} that counts the number of total steps taken during the learning process, and then pass this \texttt{SymbolCounterSUL} to the membershipOracle. After the experiment, obtain the data using the \texttt{getStatisticalData()} method. Observe the output.

Now we want to see if we can improve this. Learnlib offers a caching mechanism for SULs that might just help. Read the documentation to see what the cache wrapper\footnote{\url{https://github.com/LearnLib/learnlib/blob/develop/filters/cache/src/main/java/de/learnlib/cache/sul/SULCache.java}} does:


Since \texttt{SULCache}'s constructor method is deprecated, we recommend that you use the \texttt{createCache()} method from the \texttt{SULCaches}\footnote{\url{https://github.com/LearnLib/learnlib/blob/develop/filters/cache/src/main/java/de/learnlib/cache/sul/SULCaches.java}} class to wrap your \texttt{SUL} with this caching mechanism. 


Think about whether you should wrap the \texttt{SUL} directly or whether you should wrap the \texttt{SymbolCounterSUL}. Hint: the \texttt{SymbolCounterSUL} counts every call of the \texttt{step()} method before it passes on the call to the \texttt{SUL} it wraps. If you cannot determine the correct order, you can always experiment to find out :)

See if, after wrapping the \texttt{SUL} in a cache, you can notice a significant change in the number of executed steps.

\section*{Part 3. Learning Experiment}
For this part of the assignment you will experiment with different learning algorithms and hypothesis testing methods. To save you from the bulk of the work of setting this experiment up with Learnlib, the experiments setup is given, based on a slightly modified version borrowed from RU Nijmegen\footnote{\url{https://gitlab.science.ru.nl/ramonjanssen/basic-learning}}. 

Download the code for this experiment setup from the GitHub repository. In order to run this experiment, you will also require an updated version of the learnlib jar. Look for the Learnlib-with-ETF-Updted.jar file in the lib folder, and include this (instead of the old jar) in your project. You should now be able to run the experiment using the Experiment\_Main class.

In this setup, the experiment is given the learning algorithm and hypothesis testing method as parameters. These enum parameters can easily be changed between runs of the experiments. In this assignment it is your task to fiddle around with these to find the optimal combination. Perform this task for both the ExperimentSUL included in the experiment and your own WGCFSUL. 

\begin{itemize}
     \item What is the fastest \emph{learning} algorithm? 
     \item What is the fastest \emph{testing} algorithm? 
     \item What is the minimum value for \texttt{maxDepth} in the \texttt{WMethodEQOracle}/\texttt{WpMethodEQOracle} that still gives you a correct final hypothesis?
     \item How well does the \texttt{RandomWalkEQOracle} perform with different parameters?
\end{itemize}

\section*{Part 4. Model checking}
In this section we will show how a learned automaton can be checked for correctness, through Linear Temporal Logic (LTL).
\begin{enumerate}
     \item     The LearnLib itself is not capable of model checking a learned automaton, therefore we are going to use LTSmin.
               LTSmin requires a Mealy machine specified in \texttt{ETF} format. 
               In addition to \texttt{.dot} files, you can also export \texttt{.etf}, by adding to \texttt{Experiment\_Learner.produceOutput()},
               the following piece of code:
               
               \texttt{PrintWriter etfWriter = new PrintWriter(fileName + ".etf"); \\
                       ETF.export(model, alphabet, etfWriter); \\
                       etfWriter.close();}.
     \item     The LTSmin binaries that have been made available on Blackboard (Assignments \textgreater{} Lab \textgreater{} LTSmin) are currently only available on Linux.
               If you are not using Linux, the easiest way to get started is to download VirtualBox\footnote{\url{https://www.virtualbox.org/}}, and download a Linux virtual
               machine from \url{http://www.osboxes.org/}. Installing VirtualBox guest additions is recommended for quickly moving files (such as \texttt{.etf}) between the host and the guest.
               To do so, make sure to enable the bi-directional clip board (via \emph{Devices} in the top menu of the VM).
     \item     To instantiate the state space of a learned automaton, open up a Linux terminal, run \texttt{./etf2lts-seq learnedModel.etf}.
               This should show \texttt{state space 8 levels, 9 states 36 transitions}.
     \item     To model check an automaton add the \texttt{--ltl} option, e.g. to check the requirement ``\emph{The game can only be finished while moving the goat}'', run
               \texttt{./etf2lts-seq learnedModel.etf --ltl='[](output == "finished!" -> input == "goat?")'}.
     \item     To verify that no counter example has been found run \texttt{echo \$?}, this returns the exit code of the last command line; \texttt{0} means
               no counter example, \texttt{1} means a counter example has been found.
     \item     To show a counter example you can for example run \texttt{./etf2lts-seq learnedModel.etf --ltl='[](output == "finished!" -> input == "nothing?")'}
               (the game can only be finished while moving nothing). The commandline \texttt{echo \$?} should show \texttt{1}. To write the counter example
               to disk, add the option \texttt{--trace="/tmp/trace.gcf"}, 
               e.g. \texttt{./etf2lts-seq learnedModel.etf --ltl='[](output == "finished!" -> input == "nothing?")' \\ --trace=/tmp/trace.gcf}.
               To print the counter example, run \texttt{./ltsmin-printtrace /tmp/trace.gcf}. Aside from some verbose output, you should be able to recognize the
               inputs and outputs in the trace. As a last remark note that LTSmin will always try to find infinite traces. This is shown in the trace as a self loop, in the
               very last transition.
     \item     If you want to write your own LTL formula, note that you should encapsulate the formula in single quotes. Strings (such as "eaten!") should have double quotes.
               The Future and Globally operator are written as \texttt{<>} and \texttt{[]} respectively, and the implication as \texttt{->}.
               For more LTL operators please refer to \url{http://jmeijer.nl/ltsmin/assets/man/ltsmin-ltl.html}.
     \item     Can you spot (or prove) errors (or their absence) in WGCF implementations of other students, using active learning and model checking? The student mutants can be found on Blackboard
               (assignments \textgreater{} Lab \textgreater{} student mutants).
\end{enumerate}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
